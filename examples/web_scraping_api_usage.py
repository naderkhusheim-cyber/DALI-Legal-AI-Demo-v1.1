#!/usr/bin/env python3
"""
Web Scraping API Usage Examples
"""

import sys
import os
import requests
import json
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

def test_web_scraping_api():
    """Test the Web Scraping API endpoints"""
    print("ğŸŒ Testing Web Scraping API...\n")
    
    # Base URL for the API
    base_url = "http://localhost:8000/api/web-scraping"
    
    # Test 1: Scraping Status
    print("1ï¸âƒ£ Scraping Services Status:")
    try:
        response = requests.get(f"{base_url}/scraping-status", timeout=5)
        if response.status_code == 200:
            status = response.json()
            print(f"   âœ… Firecrawl: {status['status']['firecrawl_available']}")
            print(f"   âœ… Ollama: {status['status']['ollama_available']}")
            print(f"   ğŸ”‘ API Key: {status['status']['firecrawl_api_key_configured']}")
        else:
            print(f"   âŒ Status check failed: {response.status_code}")
    except requests.exceptions.ConnectionError:
        print("   âš ï¸ API server not running. Start with: python main.py")
        return
    except Exception as e:
        print(f"   âŒ Status check error: {e}")
        return
    
    # Test 2: Test Firecrawl Connection
    print("\n2ï¸âƒ£ Firecrawl Connection Test:")
    try:
        response = requests.get(f"{base_url}/test-firecrawl")
        if response.status_code == 200:
            result = response.json()
            print(f"   ğŸ“¡ Available: {result['firecrawl_available']}")
            print(f"   ğŸ”‘ API Key: {result['api_key_configured']}")
            print(f"   ğŸŒ Base URL: {result['base_url']}")
        else:
            print(f"   âŒ Firecrawl test failed: {response.status_code}")
    except Exception as e:
        print(f"   âŒ Firecrawl test error: {e}")
    
    # Test 3: Test Ollama Connection
    print("\n3ï¸âƒ£ Ollama Connection Test:")
    try:
        response = requests.get(f"{base_url}/test-ollama")
        if response.status_code == 200:
            result = response.json()
            print(f"   ğŸ¤– Available: {result['ollama_available']}")
            print(f"   ğŸŒ Base URL: {result['base_url']}")
            print(f"   ğŸ“Š Model: {result['analysis_model']}")
        else:
            print(f"   âŒ Ollama test failed: {response.status_code}")
    except Exception as e:
        print(f"   âŒ Ollama test error: {e}")
    
    # Test 4: Get Legal Targets
    print("\n4ï¸âƒ£ Legal Targets:")
    try:
        response = requests.get(f"{base_url}/legal-targets")
        if response.status_code == 200:
            targets = response.json()
            print(f"   âœ… Found {targets['total_targets']} legal targets")
            print(f"   ğŸ“Š Categories: {targets['total_categories']}")
            
            for category, sites in targets['legal_targets'].items():
                print(f"      {category}: {len(sites)} sites")
        else:
            print(f"   âŒ Legal targets failed: {response.status_code}")
    except Exception as e:
        print(f"   âŒ Legal targets error: {e}")
    
    # Test 5: Get Sample Queries
    print("\n5ï¸âƒ£ Sample Scraping Queries:")
    try:
        response = requests.get(f"{base_url}/sample-scraping-queries")
        if response.status_code == 200:
            queries = response.json()
            print(f"   âœ… Found {queries['total_queries']} sample queries")
            
            for i, query in enumerate(queries['sample_queries'][:2], 1):
                print(f"      {i}. {query['query']}")
                print(f"         URL: {query['url']}")
                print(f"         Method: {query['method']}")
        else:
            print(f"   âŒ Sample queries failed: {response.status_code}")
    except Exception as e:
        print(f"   âŒ Sample queries error: {e}")
    
    # Test 6: Basic Website Scraping
    print("\n6ï¸âƒ£ Basic Website Scraping:")
    try:
        scrape_data = {
            "url": "https://example.com",
            "method": "beautifulsoup",
            "options": {"timeout": 10}
        }
        
        response = requests.post(f"{base_url}/scrape", json=scrape_data)
        if response.status_code == 200:
            result = response.json()
            print(f"   âœ… Scraping successful")
            print(f"   ğŸŒ URL: {result['url']}")
            print(f"   ğŸ”§ Method: {result['method']}")
            print(f"   ğŸ“„ Content length: {result['content_length']} characters")
            print(f"   ğŸ”— Links found: {len(result.get('links', []))}")
            
            if 'analysis' in result and result['analysis']:
                analysis = result['analysis']
                print(f"   ğŸ“Š Content type: {analysis.get('content_type', 'Unknown')}")
                print(f"   âš–ï¸ Legal relevance: {analysis.get('legal_relevance', 'Unknown')}")
        else:
            print(f"   âŒ Scraping failed: {response.status_code}")
            print(f"   ğŸ“ Error: {response.text}")
    except Exception as e:
        print(f"   âŒ Scraping error: {e}")
    
    # Test 7: Document Search
    print("\n7ï¸âƒ£ Document Search:")
    try:
        doc_search_data = {
            "url": "https://example.com",
            "doc_types": ["pdf", "doc", "docx", "txt"]
        }
        
        response = requests.post(f"{base_url}/find-documents", json=doc_search_data)
        if response.status_code == 200:
            result = response.json()
            print(f"   âœ… Document search completed")
            print(f"   ğŸŒ URL: {result['url']}")
            print(f"   ğŸ“„ Documents found: {result['documents_found']}")
            
            if result['documents']:
                for doc in result['documents'][:3]:  # Show first 3
                    print(f"      - {doc['filename']} ({doc['type']})")
        else:
            print(f"   âŒ Document search failed: {response.status_code}")
    except Exception as e:
        print(f"   âŒ Document search error: {e}")
    
    # Test 8: Batch Scraping
    print("\n8ï¸âƒ£ Batch Scraping:")
    try:
        batch_data = {
            "urls": [
                "https://example.com",
                "https://httpbin.org/html",
                "https://httpbin.org/json"
            ],
            "method": "beautifulsoup"
        }
        
        response = requests.post(f"{base_url}/batch-scrape", json=batch_data)
        if response.status_code == 200:
            result = response.json()
            print(f"   âœ… Batch scraping completed")
            print(f"   ğŸ“Š Total URLs: {result['total_urls']}")
            print(f"   âœ… Successful: {result['successful_scrapes']}")
            print(f"   âŒ Failed: {result['failed_scrapes']}")
            
            for scrape in result['results'][:2]:  # Show first 2 results
                print(f"      - {scrape['url']}: {scrape['content_length']} chars")
        else:
            print(f"   âŒ Batch scraping failed: {response.status_code}")
    except Exception as e:
        print(f"   âŒ Batch scraping error: {e}")
    
    # Test 9: Legal Analysis
    print("\n9ï¸âƒ£ Legal Analysis:")
    try:
        analysis_data = {
            "url": "https://example.com",
            "analysis_type": "legal_relevance"
        }
        
        response = requests.post(f"{base_url}/legal-analysis", json=analysis_data)
        if response.status_code == 200:
            result = response.json()
            print(f"   âœ… Legal analysis completed")
            print(f"   ğŸŒ URL: {result['url']}")
            print(f"   ğŸ“Š Analysis type: {result['analysis_type']}")
            print(f"   ğŸ“„ Content length: {result['content_length']}")
            
            analysis = result['enhanced_analysis']
            print(f"   ğŸ“ˆ Relevance score: {analysis.get('relevance_score', 'N/A')}")
            print(f"   ğŸ·ï¸ Key concepts: {', '.join(analysis.get('key_legal_concepts', [])[:3])}")
        else:
            print(f"   âŒ Legal analysis failed: {response.status_code}")
            print(f"   ğŸ“ Error: {response.text}")
    except Exception as e:
        print(f"   âŒ Legal analysis error: {e}")
    
    # Test 10: Case Analysis
    print("\nğŸ”Ÿ Case Analysis:")
    try:
        case_analysis_data = {
            "url": "https://example.com",
            "analysis_type": "case_analysis"
        }
        
        response = requests.post(f"{base_url}/legal-analysis", json=case_analysis_data)
        if response.status_code == 200:
            result = response.json()
            print(f"   âœ… Case analysis completed")
            
            analysis = result['enhanced_analysis']
            print(f"   ğŸ“Š Case indicators: {', '.join(analysis.get('case_indicators', [])[:3])}")
            print(f"   ğŸ¢ Legal entities: {', '.join(analysis.get('legal_entities', [])[:3])}")
            print(f"   ğŸ“‹ Case status: {analysis.get('case_status', 'Unknown')}")
            print(f"   âš–ï¸ Jurisdiction: {analysis.get('jurisdiction', 'Unknown')}")
        else:
            print(f"   âŒ Case analysis failed: {response.status_code}")
    except Exception as e:
        print(f"   âŒ Case analysis error: {e}")
    
    print("\nâœ… Web Scraping API testing completed!")

if __name__ == "__main__":
    test_web_scraping_api()
