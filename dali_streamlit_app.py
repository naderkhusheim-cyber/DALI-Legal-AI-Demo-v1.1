#!/usr/bin/env python3
"""
DALI Legal AI MVP - Streamlit Web Interface
Development of Ambitious Leading Ideas @ Siyada Tech

A user-friendly web interface for the DALI Legal AI system
"""

import streamlit as st
import requests
import json
from datetime import datetime
import pandas as pd
from dali_technical_implementation import DALILegalAI

# Page configuration
st.set_page_config(
    page_title="DALI Legal AI",
    page_icon="âš–ï¸",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Custom CSS
st.markdown("""
<style>
    .main-header {
        font-size: 3rem;
        color: #1f4e79;
        text-align: center;
        margin-bottom: 2rem;
    }
    .sub-header {
        font-size: 1.2rem;
        color: #666;
        text-align: center;
        margin-bottom: 3rem;
    }
    .status-box {
        padding: 1rem;
        border-radius: 0.5rem;
        margin: 1rem 0;
    }
    .status-success {
        background-color: #d4edda;
        border: 1px solid #c3e6cb;
        color: #155724;
    }
    .status-error {
        background-color: #f8d7da;
        border: 1px solid #f5c6cb;
        color: #721c24;
    }
    .status-warning {
        background-color: #fff3cd;
        border: 1px solid #ffeaa7;
        color: #856404;
    }
</style>
""", unsafe_allow_html=True)

def initialize_session_state():
    """Initialize session state variables"""
    if 'dali_system' not in st.session_state:
        st.session_state.dali_system = DALILegalAI()
    if 'chat_history' not in st.session_state:
        st.session_state.chat_history = []
    if 'documents_added' not in st.session_state:
        st.session_state.documents_added = 0
    if 'language' not in st.session_state:
        st.session_state.language = 'en'  # Default to English

# Translation system
LANGUAGES = {
    'en': {
        'system_status': 'System Status',
        'ollama_running': 'Ollama: Running',
        'ollama_not_running': 'Ollama: Not Running',
        'firecrawl_configured': 'Firecrawl: Configured',
        'firecrawl_not_configured': 'Firecrawl: Not Configured',
        'vector_db_ready': 'Vector DB: Ready',
        'vector_db_error': 'Vector DB: Error',
        'documents_in_db': 'Documents in database',
        'select_model': 'Select Model',
        'no_models_available': 'No models available. Run: ollama pull llama3',
        'start_ollama': 'Please start Ollama: `ollama serve`',
        'set_firecrawl_key': 'Set FIRECRAWL_API_KEY environment variable',
        'language_toggle': 'Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©',
        'chat': 'Chat',
        'web_scraping': 'Web Scraping',
        'document_management': 'Document Management',
        'analytics': 'Analytics',
        'legal_ai_assistant': 'Legal AI Assistant',
        'ask_questions': 'Ask questions about your legal documents using natural language.',
        'ask_legal_question': 'Ask a legal question:',
        'ask_question_placeholder': 'e.g., What are the key elements of a valid contract?',
        'ask_question_button': 'Ask Question',
        'clear_chat_history': 'Clear Chat History',
        'chat_history': 'Chat History',
        'question': 'Question',
        'answer': 'Answer',
        'sources_used': 'Sources Used',
        'source_details': 'Source Details',
        'asked_at': 'Asked at',
        'legal_web_scraping': 'Legal Web Scraping',
        'scrape_description': 'Scrape legal websites and add content to your knowledge base.',
        'enter_url': 'Enter URL to scrape:',
        'scrape_placeholder': 'https://example-legal-site.com/article',
        'scrape_button': 'Scrape Website',
        'add_to_kb': 'Add to knowledge base',
        'firecrawl_not_configured_error': 'Firecrawl API key not configured. Please set FIRECRAWL_API_KEY environment variable.',
        'scraping_website': 'Scraping website...',
        'scraping_failed': 'Scraping failed',
        'website_scraped_successfully': 'Website scraped successfully!',
        'scraped_content_preview': 'Scraped Content Preview',
        'content': 'Content',
        'content_added_to_kb': 'Content added to knowledge base!',
        'failed_to_add_content': 'Failed to add content to knowledge base.',
        'upload_manage_documents': 'Upload and manage legal documents in your knowledge base.',
        'upload_legal_documents': 'Upload legal documents',
        'file_limit': 'Max 200MB per file â€¢ PDF, DOCX, TXT, MD',
        'pdf_processing_not_implemented': 'PDF processing not implemented in MVP. Please convert to text first.',
        'unsupported_file_type': 'Unsupported file type',
        'content_preview': 'Content preview',
        'add_to_knowledge_base': 'Add to Knowledge Base',
        'added_to_kb': 'added to knowledge base!',
        'failed_to_add_to_kb': 'Failed to add to knowledge base.',
        'search_documents': 'Search Documents',
        'search_in_kb': 'Search in knowledge base:',
        'searching_documents': 'Searching documents...',
        'found_documents': 'Found relevant documents:',
        'document': 'Document',
        'similarity': 'Similarity',
        'no_documents_found': 'No documents found matching your search.',
        'system_analytics': 'System Analytics',
        'documents_added': 'Documents Added',
        'total_documents': 'Total Documents',
        'chat_sessions': 'Chat Sessions',
        'available_models': 'Available Models',
        'available_models_title': 'Available Models',
        'no_models_available_install': 'No models available. Install models using: `ollama pull <model_name>`',
        'recent_activity': 'Recent Activity',
        'no_recent_activity': 'No recent activity.',
        'timestamp': 'Timestamp',
        'sources_used_count': 'Sources Used',
        'footer_text': 'DALI Legal AI MVP - Development of Ambitious Leading Ideas @ Siyada Tech | Built with â¤ï¸ using Ollama, Firecrawl, and Chroma'
    },
    'ar': {
        'system_status': 'Ø­Ø§Ù„Ø© Ø§Ù„Ù†Ø¸Ø§Ù…',
        'ollama_running': 'Ø£ÙˆÙ„Ø§Ù…Ø§: ÙŠØ¹Ù…Ù„',
        'ollama_not_running': 'Ø£ÙˆÙ„Ø§Ù…Ø§: Ù„Ø§ ÙŠØ¹Ù…Ù„',
        'firecrawl_configured': 'ÙØ§ÙŠØ±ÙƒØ±ÙˆÙ„: Ù…ÙÙƒÙˆÙÙ‘Ù†',
        'firecrawl_not_configured': 'ÙØ§ÙŠØ±ÙƒØ±ÙˆÙ„: ØºÙŠØ± Ù…ÙÙƒÙˆÙÙ‘Ù†',
        'vector_db_ready': 'Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªØ¬Ù‡Ø©: Ø¬Ø§Ù‡Ø²Ø©',
        'vector_db_error': 'Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªØ¬Ù‡Ø©: Ø®Ø·Ø£',
        'documents_in_db': 'Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª',
        'select_model': 'Ø§Ø®ØªØ± Ø§Ù„Ù†Ù…ÙˆØ°Ø¬',
        'no_models_available': 'Ù„Ø§ ØªÙˆØ¬Ø¯ Ù†Ù…Ø§Ø°Ø¬ Ù…ØªØ§Ø­Ø©. ØªØ´ØºÙŠÙ„: ollama pull llama3',
        'start_ollama': 'ÙŠØ±Ø¬Ù‰ ØªØ´ØºÙŠÙ„ Ø£ÙˆÙ„Ø§Ù…Ø§: `ollama serve`',
        'set_firecrawl_key': 'Ù‚Ù… Ø¨ØªØ¹ÙŠÙŠÙ† Ù…ØªØºÙŠØ± Ø§Ù„Ø¨ÙŠØ¦Ø© FIRECRAWL_API_KEY',
        'language_toggle': 'English',
        'chat': 'Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©',
        'web_scraping': 'Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ÙˆÙŠØ¨',
        'document_management': 'Ø¥Ø¯Ø§Ø±Ø© Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª',
        'analytics': 'Ø§Ù„ØªØ­Ù„ÙŠÙ„Ø§Øª',
        'legal_ai_assistant': 'Ù…Ø³Ø§Ø¹Ø¯ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠ',
        'ask_questions': 'Ø§Ø·Ø±Ø­ Ø£Ø³Ø¦Ù„Ø© Ø­ÙˆÙ„ Ù…Ø³ØªÙ†Ø¯Ø§ØªÙƒ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ©.',
        'ask_legal_question': 'Ø§Ø·Ø±Ø­ Ø³Ø¤Ø§Ù„Ø§Ù‹ Ù‚Ø§Ù†ÙˆÙ†ÙŠØ§Ù‹:',
        'ask_question_placeholder': 'Ù…Ø«Ø§Ù„: Ù…Ø§ Ù‡ÙŠ Ø§Ù„Ø¹Ù†Ø§ØµØ± Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ© Ù„Ù„Ø¹Ù‚Ø¯ Ø§Ù„ØµØ­ÙŠØ­ØŸ',
        'ask_question_button': 'Ø§Ø·Ø±Ø­ Ø§Ù„Ø³Ø¤Ø§Ù„',
        'clear_chat_history': 'Ù…Ø³Ø­ Ø³Ø¬Ù„ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©',
        'chat_history': 'Ø³Ø¬Ù„ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©',
        'question': 'Ø§Ù„Ø³Ø¤Ø§Ù„',
        'answer': 'Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©',
        'sources_used': 'Ø§Ù„Ù…ØµØ§Ø¯Ø± Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…Ø©',
        'source_details': 'ØªÙØ§ØµÙŠÙ„ Ø§Ù„Ù…ØµØ¯Ø±',
        'asked_at': 'Ø·ÙÙ„Ø¨ ÙÙŠ',
        'legal_web_scraping': 'Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ÙˆÙŠØ¨ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠ',
        'scrape_description': 'Ø§Ø³ØªØ®Ø±Ø¬ Ù…ÙˆØ§Ù‚Ø¹ Ø§Ù„ÙˆÙŠØ¨ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© ÙˆØ£Ø¶Ù Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø¥Ù„Ù‰ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø¹Ø±ÙØ© Ø§Ù„Ø®Ø§ØµØ© Ø¨Ùƒ.',
        'enter_url': 'Ø£Ø¯Ø®Ù„ Ø±Ø§Ø¨Ø· Ø§Ù„Ù…ÙˆÙ‚Ø¹ Ù„Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬:',
        'scrape_placeholder': 'https://example-legal-site.com/article',
        'scrape_button': 'Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…ÙˆÙ‚Ø¹',
        'add_to_kb': 'Ø£Ø¶Ù Ø¥Ù„Ù‰ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø¹Ø±ÙØ©',
        'firecrawl_not_configured_error': 'Ù…ÙØªØ§Ø­ API Ù„ÙØ§ÙŠØ±ÙƒØ±ÙˆÙ„ ØºÙŠØ± Ù…ÙÙƒÙˆÙÙ‘Ù†. ÙŠØ±Ø¬Ù‰ ØªØ¹ÙŠÙŠÙ† Ù…ØªØºÙŠØ± Ø§Ù„Ø¨ÙŠØ¦Ø© FIRECRAWL_API_KEY.',
        'scraping_website': 'Ø¬Ø§Ø±ÙŠ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…ÙˆÙ‚Ø¹...',
        'scraping_failed': 'ÙØ´Ù„ Ø§Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬',
        'website_scraped_successfully': 'ØªÙ… Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…ÙˆÙ‚Ø¹ Ø¨Ù†Ø¬Ø§Ø­!',
        'scraped_content_preview': 'Ù…Ø¹Ø§ÙŠÙ†Ø© Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ù…Ø³ØªØ®Ø±Ø¬',
        'content': 'Ø§Ù„Ù…Ø­ØªÙˆÙ‰',
        'content_added_to_kb': 'ØªÙ… Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø¥Ù„Ù‰ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø¹Ø±ÙØ©!',
        'failed_to_add_content': 'ÙØ´Ù„ ÙÙŠ Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø¥Ù„Ù‰ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø¹Ø±ÙØ©.',
        'upload_manage_documents': 'Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ ÙˆØ¥Ø¯Ø§Ø±Ø© Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø¹Ø±ÙØ© Ø§Ù„Ø®Ø§ØµØ© Ø¨Ùƒ.',
        'upload_legal_documents': 'ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ©',
        'file_limit': 'Ø§Ù„Ø­Ø¯ Ø§Ù„Ø£Ù‚ØµÙ‰ 200 Ù…ÙŠØ¬Ø§Ø¨Ø§ÙŠØª Ù„ÙƒÙ„ Ù…Ù„Ù â€¢ PDF, DOCX, TXT, MD',
        'pdf_processing_not_implemented': 'Ù…Ø¹Ø§Ù„Ø¬Ø© PDF ØºÙŠØ± Ù…ÙÙ†ÙÙÙ‘Ø°Ø© ÙÙŠ MVP. ÙŠØ±Ø¬Ù‰ Ø§Ù„ØªØ­ÙˆÙŠÙ„ Ø¥Ù„Ù‰ Ù†Øµ Ø£ÙˆÙ„Ø§Ù‹.',
        'unsupported_file_type': 'Ù†ÙˆØ¹ Ø§Ù„Ù…Ù„Ù ØºÙŠØ± Ù…Ø¯Ø¹ÙˆÙ…',
        'content_preview': 'Ù…Ø¹Ø§ÙŠÙ†Ø© Ø§Ù„Ù…Ø­ØªÙˆÙ‰',
        'add_to_knowledge_base': 'Ø£Ø¶Ù Ø¥Ù„Ù‰ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø¹Ø±ÙØ©',
        'added_to_kb': 'ØªÙ… Ø¥Ø¶Ø§ÙØ© Ø¥Ù„Ù‰ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø¹Ø±ÙØ©!',
        'failed_to_add_to_kb': 'ÙØ´Ù„ ÙÙŠ Ø¥Ø¶Ø§ÙØ© Ø¥Ù„Ù‰ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø¹Ø±ÙØ©.',
        'search_documents': 'Ø§Ù„Ø¨Ø­Ø« ÙÙŠ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª',
        'search_in_kb': 'Ø§Ù„Ø¨Ø­Ø« ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø¹Ø±ÙØ©:',
        'searching_documents': 'Ø¬Ø§Ø±ÙŠ Ø§Ù„Ø¨Ø­Ø« ÙÙŠ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª...',
        'found_documents': 'ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù…Ø³ØªÙ†Ø¯Ø§Øª Ø°Ø§Øª ØµÙ„Ø©:',
        'document': 'Ø§Ù„Ù…Ø³ØªÙ†Ø¯',
        'similarity': 'Ø§Ù„ØªØ´Ø§Ø¨Ù‡',
        'no_documents_found': 'Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù…Ø³ØªÙ†Ø¯Ø§Øª ØªØ·Ø§Ø¨Ù‚ Ø¨Ø­Ø«Ùƒ.',
        'system_analytics': 'ØªØ­Ù„ÙŠÙ„Ø§Øª Ø§Ù„Ù†Ø¸Ø§Ù…',
        'documents_added': 'Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª Ø§Ù„Ù…Ø¶Ø§ÙØ©',
        'total_documents': 'Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª',
        'chat_sessions': 'Ø¬Ù„Ø³Ø§Øª Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©',
        'available_models': 'Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù…ØªØ§Ø­Ø©',
        'available_models_title': 'Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù…ØªØ§Ø­Ø©',
        'no_models_available_install': 'Ù„Ø§ ØªÙˆØ¬Ø¯ Ù†Ù…Ø§Ø°Ø¬ Ù…ØªØ§Ø­Ø©. Ù‚Ù… Ø¨ØªØ«Ø¨ÙŠØª Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù…: `ollama pull <model_name>`',
        'recent_activity': 'Ø§Ù„Ù†Ø´Ø§Ø· Ø§Ù„Ø£Ø®ÙŠØ±',
        'no_recent_activity': 'Ù„Ø§ ÙŠÙˆØ¬Ø¯ Ù†Ø´Ø§Ø· Ø­Ø¯ÙŠØ«.',
        'timestamp': 'Ø§Ù„ÙˆÙ‚Øª',
        'sources_used_count': 'Ø§Ù„Ù…ØµØ§Ø¯Ø± Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…Ø©',
        'footer_text': 'Ø¯Ø§Ù„ÙŠ Ù„Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠ MVP - ØªØ·ÙˆÙŠØ± Ø§Ù„Ø£ÙÙƒØ§Ø± Ø§Ù„Ø·Ù…ÙˆØ­Ø© Ø§Ù„Ø±Ø§Ø¦Ø¯Ø© @ Ø³ÙŠØ¯Ø© ØªÙƒ | Ù…Ø¨Ù†ÙŠ Ø¨Ù€ â¤ï¸ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£ÙˆÙ„Ø§Ù…Ø§ ÙˆÙØ§ÙŠØ±ÙƒØ±ÙˆÙ„ ÙˆÙƒØ±ÙˆÙ…'
    }
}

def t(key):
    """Translation function"""
    lang = st.session_state.language
    return LANGUAGES[lang].get(key, key)

def display_system_status():
    """Display system status in sidebar"""
    # Language toggle at the top
    st.sidebar.markdown("## ğŸŒ Language / Ø§Ù„Ù„ØºØ©")
    lang_toggle_label = t('language_toggle')
    if st.sidebar.button(lang_toggle_label, key='lang_toggle', help='Switch language / ØªØ¨Ø¯ÙŠÙ„ Ø§Ù„Ù„ØºØ©'):
        st.session_state.language = 'ar' if st.session_state.language == 'en' else 'en'
        st.rerun()
    
    st.sidebar.markdown("## ğŸ”§ " + t('system_status'))
    
    dali = st.session_state.dali_system
    
    # Check Ollama status
    ollama_status = dali.check_ollama_status()
    if ollama_status:
        st.sidebar.markdown('<div class="status-box status-success">âœ… ' + t('ollama_running') + '</div>', unsafe_allow_html=True)
        models = dali.list_available_models()
        if models:
            st.sidebar.selectbox(t('select_model'), models, key="selected_model")
        else:
            st.sidebar.warning(t('no_models_available'))
    else:
        st.sidebar.markdown('<div class="status-box status-error">âŒ ' + t('ollama_not_running') + '</div>', unsafe_allow_html=True)
        st.sidebar.error(t('start_ollama'))
    
    # Check Firecrawl API
    if dali.firecrawl_api_key:
        st.sidebar.markdown('<div class="status-box status-success">âœ… ' + t('firecrawl_configured') + '</div>', unsafe_allow_html=True)
    else:
        st.sidebar.markdown('<div class="status-box status-warning">âš ï¸ ' + t('firecrawl_not_configured') + '</div>', unsafe_allow_html=True)
        st.sidebar.info(t('set_firecrawl_key'))
    
    # Vector database status
    try:
        collection_count = st.session_state.dali_system.collection.count()
        st.sidebar.markdown('<div class="status-box status-success">âœ… ' + t('vector_db_ready') + '</div>', unsafe_allow_html=True)
        st.sidebar.info(t('documents_in_db') + f": {collection_count}")
    except:
        st.sidebar.markdown('<div class="status-box status-error">âŒ ' + t('vector_db_error') + '</div>', unsafe_allow_html=True)

def main_interface():
    """Main application interface"""
    
    # Header
    st.markdown('<h1 class="main-header">âš–ï¸ DALI Legal AI</h1>', unsafe_allow_html=True)
    st.markdown('<p class="sub-header">Development of Ambitious Leading Ideas @ Siyada Tech</p>', unsafe_allow_html=True)
    
    # Main tabs
    tab1, tab2, tab3, tab4 = st.tabs(["ğŸ’¬ " + t('chat'), "ğŸŒ " + t('web_scraping'), "ğŸ“š " + t('document_management'), "ğŸ“Š " + t('analytics')])
    
    with tab1:
        chat_interface()
    
    with tab2:
        web_scraping_interface()
    
    with tab3:
        document_management_interface()
    
    with tab4:
        analytics_interface()

def chat_interface():
    """Legal AI chat interface"""
    st.markdown("## ğŸ’¬ " + t('legal_ai_assistant'))
    st.markdown(t('ask_questions'))
    
    # Chat input
    user_question = st.text_input(
        t('ask_legal_question'),
        placeholder=t('ask_question_placeholder'),
        key="chat_input"
    )
    
    col1, col2 = st.columns([1, 4])
    with col1:
        ask_button = st.button(t('ask_question_button'), type="primary")
    with col2:
        clear_chat = st.button(t('clear_chat_history'))
    
    if clear_chat:
        st.session_state.chat_history = []
        st.rerun()
    
    if ask_button and user_question:
        with st.spinner("Searching legal documents and generating response..."):
            # Get selected model
            selected_model = st.session_state.get("selected_model", "llama3.2:1b")
            
            # Perform RAG query
            result = st.session_state.dali_system.legal_rag_query(user_question, selected_model)
            
            # Add to chat history
            st.session_state.chat_history.append({
                "timestamp": datetime.now(),
                "question": user_question,
                "answer": result["answer"],
                "sources": result["sources"],
                "context_used": result["context_used"]
            })
    
    # Display chat history
    if st.session_state.chat_history:
        st.markdown("### ğŸ“ " + t('chat_history'))
        for i, chat in enumerate(reversed(st.session_state.chat_history)):
            with st.expander(f"Q: {chat['question'][:50]}..." if len(chat['question']) > 50 else f"Q: {chat['question']}", expanded=(i==0)):
                st.markdown(f"**{t('question')}:** {chat['question']}")
                st.markdown(f"**{t('answer')}:** {chat['answer']}")
                st.markdown(f"**{t('sources_used')}:** {chat['context_used']} documents")
                if chat['sources']:
                    st.markdown(f"**{t('source_details')}:**")
                    for source in chat['sources']:
                        st.json(source)
                st.markdown(f"*{t('asked_at')}: {chat['timestamp'].strftime('%Y-%m-%d %H:%M:%S')}*")

def web_scraping_interface():
    """Web scraping interface"""
    st.markdown("## ğŸŒ " + t('legal_web_scraping'))
    st.markdown(t('scrape_description'))
    
    # URL input
    url_to_scrape = st.text_input(
        t('enter_url'),
        placeholder=t('scrape_placeholder'),
        key="scrape_url"
    )
    
    col1, col2 = st.columns(2)
    with col1:
        scrape_button = st.button(t('scrape_button'), type="primary")
    with col2:
        add_to_db = st.checkbox(t('add_to_kb'), value=True)
    
    if scrape_button and url_to_scrape:
        if not st.session_state.dali_system.firecrawl_api_key:
            st.error(t('firecrawl_not_configured_error'))
            return
        
        with st.spinner(t('scraping_website')):
            result = st.session_state.dali_system.scrape_with_firecrawl(url_to_scrape)
            
            if "error" in result:
                st.error(f"{t('scraping_failed')}: {result['error']}")
            else:
                st.success(t('website_scraped_successfully'))
                
                # Display scraped content
                if "data" in result and "markdown" in result["data"]:
                    content = result["data"]["markdown"]
                    st.markdown("### ğŸ“„ " + t('scraped_content_preview'))
                    st.text_area(t('content'), content[:1000] + "..." if len(content) > 1000 else content, height=200)
                    
                    if add_to_db:
                        # Add to vector database
                        metadata = {
                            "source": url_to_scrape,
                            "type": "web_scraped",
                            "scraped_at": datetime.now().isoformat(),
                            "title": result["data"].get("title", "Unknown")
                        }
                        
                        if st.session_state.dali_system.add_document_to_vectordb(content, metadata):
                            st.success(t('content_added_to_kb'))
                            st.session_state.documents_added += 1
                        else:
                            st.error(t('failed_to_add_content'))

def document_management_interface():
    """Document management interface"""
    st.markdown("## ğŸ“š " + t('document_management'))
    st.markdown(t('upload_manage_documents'))
    
    # File upload
    uploaded_files = st.file_uploader(
        t('upload_legal_documents'),
        type=['txt', 'md', 'pdf'],
        accept_multiple_files=True,
        key="doc_upload"
    )
    
    if uploaded_files:
        for uploaded_file in uploaded_files:
            st.markdown(f"### ğŸ“„ {uploaded_file.name}")
            
            # Read file content
            if uploaded_file.type == "text/plain" or uploaded_file.name.endswith('.md'):
                content = str(uploaded_file.read(), "utf-8")
            elif uploaded_file.name.endswith('.pdf'):
                st.warning(t('pdf_processing_not_implemented'))
                continue
            else:
                st.error(f"{t('unsupported_file_type')}: {uploaded_file.type}")
                continue
            
            # Display content preview
            st.text_area(f"{t('content_preview')} - {uploaded_file.name}", content[:500] + "..." if len(content) > 500 else content, height=150)
            
            # Add to database button
            if st.button(f"{t('add_to_knowledge_base')} {uploaded_file.name}", key=f"add_{uploaded_file.name}"):
                metadata = {
                    "source": uploaded_file.name,
                    "type": "uploaded_document",
                    "uploaded_at": datetime.now().isoformat(),
                    "file_size": len(content)
                }
                
                if st.session_state.dali_system.add_document_to_vectordb(content, metadata):
                    st.success(f"âœ… {uploaded_file.name} {t('added_to_kb')}")
                    st.session_state.documents_added += 1
                else:
                    st.error(f"âŒ {t('failed_to_add_to_kb')} {uploaded_file.name}")
    
    # Document search
    st.markdown("### ğŸ” " + t('search_documents'))
    search_query = st.text_input(t('search_in_kb'), key="doc_search")
    
    if search_query:
        with st.spinner(t('searching_documents')):
            results = st.session_state.dali_system.search_documents(search_query, n_results=10)
            
            if results:
                st.markdown(f"{t('found_documents')} {len(results)}:")
                for i, doc in enumerate(results):
                    with st.expander(f"{t('document')} {i+1} ({t('similarity')}: {1-doc['distance']:.3f})"):
                        st.markdown(f"**{t('content')}:** {doc['content'][:300]}...")
                        st.json(doc['metadata'])
            else:
                st.info(t('no_documents_found'))

def analytics_interface():
    """Analytics and system information"""
    st.markdown("## ğŸ“Š " + t('system_analytics'))
    
    # System metrics
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        st.metric(t('documents_added'), st.session_state.documents_added)
    
    with col2:
        try:
            total_docs = st.session_state.dali_system.collection.count()
            st.metric(t('total_documents'), total_docs)
        except:
            st.metric(t('total_documents'), "Error")
    
    with col3:
        st.metric(t('chat_sessions'), len(st.session_state.chat_history))
    
    with col4:
        models = st.session_state.dali_system.list_available_models()
        st.metric(t('available_models'), len(models))
    
    # Model information
    st.markdown("### ğŸ¤– " + t('available_models_title'))
    models = st.session_state.dali_system.list_available_models()
    if models:
        model_df = pd.DataFrame({"Model Name": models})
        st.dataframe(model_df, use_container_width=True)
    else:
        st.info(t('no_models_available_install'))
    
    # Recent activity
    st.markdown("### ğŸ“ˆ " + t('recent_activity'))
    if st.session_state.chat_history:
        recent_chats = st.session_state.chat_history[-5:]  # Last 5 chats
        activity_data = []
        for chat in recent_chats:
            activity_data.append({
                t('timestamp'): chat["timestamp"].strftime("%Y-%m-%d %H:%M:%S"),
                t('question'): chat["question"][:50] + "..." if len(chat["question"]) > 50 else chat["question"],
                t('sources_used_count'): chat["context_used"]
            })
        
        activity_df = pd.DataFrame(activity_data)
        st.dataframe(activity_df, use_container_width=True)
    else:
        st.info(t('no_recent_activity'))

def main():
    """Main application entry point"""
    initialize_session_state()
    display_system_status()
    main_interface()
    
    # Footer
    st.markdown("---")
    st.markdown(t('footer_text'))

if __name__ == "__main__":
    main()

